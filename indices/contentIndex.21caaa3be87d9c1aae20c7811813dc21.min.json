{"/":{"title":"Home","content":"\nHowdy. I'm Nikola. This is my website.\n\n","lastmodified":"2022-09-10T18:41:36.538594346Z","tags":null},"/notes/Exact-Inference":{"title":"Exact Inference","content":"\nGiven samples from a probabilistic model of a system, a frequent research question is to infer certain other properties about the system, or otherwise reduce the uncertainty in the sampled information - as in sensor fusion problems. In certain scenarios, if we make the right assumptions, we can compute exact, deterministic solutions. A classical example of such an algorithm is the Kalman filter, which generalizes to [[notes/Gaussian Belief Propagation (GBP)]].","lastmodified":"2022-09-10T18:41:36.542594395Z","tags":null},"/notes/Gaussian-Belief-Propagation-GBP":{"title":"Gaussian Belief Propagation (GBP)","content":"\nWith an understanding of Gaussian algebra, [[notes/Graphical Models]], tree decomposition, and variable elimination, it becomes apparent that GBP is nothing more than a clever application of the distributive law.\n\nGraphical models also known as [[notes/Graphical Models|PGMs]].\n\nThe factor graphs are very interesting. [[notes/Graphical Models#Factor Graphs|kek]]\n\nAn interesting application is in SLAM algorithms with probabilistic model represented by a factor graph.\n\n![[visuals/factor_graph_slam.png]]","lastmodified":"2022-09-10T18:41:36.542594395Z","tags":null},"/notes/Graphical-Models":{"title":"Graphical Models","content":"There are a variety of graphical models, which explicitly represent the structure of a joint PDF.  Three popular variants are described below. The more general framework of describing interaction between nodes - think particles - using graphs is extremely versatile.\n\n### Bayesian Networks\nA Bayesian network $\\mathcal{B}=(V,E)$ is a directed acyclic graph (DAG). Its nodes $V = \\{X_1\\dots X_n\\}$ represent random variables and the edges $E \\subset V\\times V$ show conditional dependencies. It encodes a probability distribution where factors are either a prior of a variable, or a posterior conditioned on its parents. If we denote the parent nodes of $X_i$ as $\\pi_i$, the joint PDF over $n$ variables is defined as\n$$p_{X_1\\dots X_n}(x_1\\dots x_n) = \\prod_i p(x_i|\\pi_i)p(x_i)$$\n\n### Markov Random Fields\nA Markov random field (MRF) is an undirected graph $\\mathcal{M}=(V,E)$, which encodes conditional independence between variables in a joint PDF. For example, in a three node graph $X-Z-Y$, the joint probability is $p_{XYZ}(x,y,z) = p_{X|Z}(x;z)p_{Y|Z}(y;z)p_Z(z)$. In particular, since all paths between nodes $X$ and $Y$ on the graph pass through node $Z$, we deduce that $X$ and Y are independent conditioned on $Z$. This is similar to the Markov property and motivates a factorization over cliques $\\{(X,Z),(Y,Z)\\}$. A clique is a complete induced subgraph and is maximal when it is not contained within a larger clique. If nodes in $V$ are grouped into a set of maximal cliques $C$, the joint probability represented by the undirected graph can be written as a product of potential functions with inputs in $C$ and normalizing partition function $Z$.\n$$p_{X_1\\dots X_n}(x_1\\dots x_n) =\\frac{1}{Z}\\prod_{c\\,\\in C} \\psi_{c}(x_{c}) $$\nA DAG can be converted to an undirected graph through moralization -- fully connecting each node's parents. This can result in loss of some conditional independencies though.\n\n### Factor Graphs\nFactor graph $\\mathcal{F}=(V,E,F)$ is an undirected bipartite graph with edges $E\\subseteq V\\times F$ between nodes $V$ representing random variables and nodes $F$ for factors, making the factorization explicit. A variable connected to a factor means it is given as input to the factor function. Hence, the joint PDF with normalizing partition function $Z$ can be expressed as:\n$$p_{X_1\\dots X_n}(x_1\\dots x_n) =\\frac{1}{Z}\\prod_{\\phi\\,\\in F} \\phi(x_{\\phi})$$","lastmodified":"2022-09-10T18:41:36.542594395Z","tags":null},"/notes/Variational-Inference":{"title":"Variational Inference","content":"\nWhen a problem is called *variational*, think approximate. When [[notes/Exact Inference]] is intractable, we consider a variational approach.","lastmodified":"2022-09-10T18:41:36.542594395Z","tags":null}}